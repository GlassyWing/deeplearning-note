# 卷积神经网络

## 填充

为了防止多次过滤后，图片变得很小，同时为了减少图片边界信息的丢失，需要对图片进行填充。
填充后的图片大小变为：(n + 2p) x (n+2p)
那么经过过滤后的图片大小变为：(n + 2p - f + 1) x (n + 2p - f + 1)

填充多少像素基本有两个选择：

1. Valid:

   不填充

2. Same:

   填充后的图片经过滤后与原图片一样大，即满足:
   $$ n + 2p -f + 1 = n $$

   $$ p = \frac{f-1}{2} $$

   那么当f是奇数时，可以实现过滤后的图片与原图一样大。

## 步幅

步幅决定了核一次移动的像素，引入步长后，过滤后的图片大小计算公式如下：
$$ \frac{n+2p-f}{s} + 1 $$

## 多个核

设输入图像大小为 $ n*n*n_c $, $n_c$为上一层图像通道数，
则每一个核的大小为 $ f *f *n_c $，
那么经过滤后的图像大小为 $ (n -f + 1)*(n -f + 1) * \dot{n_{c}} $,
$ \dot{n_{c}} $为下一层的通道数，也是核的数量。

## 池化层

1. max pooling

   最大池化层或许保留了最重要的激活值而使得模型表现更好，但具体原因并不清楚。
   它通过在图像上应用一个核，并取这个核中的最大值。

   技巧：若想让图像缩小一般，则令池化层的参数$ s = f = 2$

2. average pooling

    它通过在图像上应用一个核，并计算核中的平均值。

## 残差网络

非常深的网络面临着一个巨大的问题：梯度消失，随着层数的增加，梯度以指数级变小，使得最后梯度下降变得异常缓慢（当然，如果参数初始化很大，也有可能造成梯度爆炸）

### 捷径

捷径通过将早前的层的激活值直接加入到后面的层激活中，允许梯度直接反向传播至较前面的层，

1. 恒等块
    残差网络通过残差块可以学到一个恒等函数（残差块的激活函数需为Relu），这意味前面的激活值与后面的激活值维度相同
2. 卷积块
   卷积块是为了应对前面的激活与后面的激活值维度不同而创建的