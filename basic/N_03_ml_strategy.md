# 机器学习策略

## 迭代流程

1. 建立指标
2. 评估指标
3. 根据指标选择模型
4. 验证模型
5. 重新评估
6. 优化模型

## 建立指标

### 使用单一量化评估指标

在比对模型时，若存在多个指标，无法对模型进行分析的情况下，可参考综合多个指标的指标，例如：

1. 在参考精确度和召回率无法给出指示性操作的前提下，可使用F1值作为参考指标，F1值根据精确度和召回率得出，公式为$ F1 = \frac{2}{\frac{1}{P} + \frac{1}{R}} $。

    | Classfier | Precision | Recall | F1 Score |
    | --------- | --------- | ------ | -------- |
    | A         | 95%       | 90%    | 92.4%    |
    | B         | 98%       | 85%    | 91.0%    |

2. 假设猫分类器在各个地区的误差率如下，分类器在不同地区有不同的错误率，这时可以使用平均值作为单一量化指标：

    | Algorithm | US  | China | India | Other | Average |
    | --------- | --- | ----- | ----- | ----- | ------- |
    | A         | 3%  | 7%    | 5%    | 9%    | 6%      |
    | B         | 5%  | 6%    | 5%    | 10%   | 6.5%    |

### 建立满足指标

以下是猫分类器以及我们所关心的指标，若使用单一指标，例如$ cost = accuracy -0.5 * runningTime $，这有些刻意，并且只是线性变换。

| Classfier | Accuracy | Running time |
| --------- | -------- | ------------ |
| A         | 90%      | 80ms         |
| B         | 92%      | 95ms         |
| c         | 95%      | 1500ms       |

因此，可以这样考虑，在保证运行时间的前提下，提供最大精确度的模型是我们想要的模型。这里，运行时间即我们的满足指标。


## 评估指标调整

1. 评估指标无法正确选择模型时，例如以下两个猫分类器，按照评估指标，A是更好的选择，但会为用户展示色情图片，而B的误差率较高，但不会展示色情图片，这对公司和用户来说都是更好的选择，因此此时应该调整评估指标（可能也要修改开发集或测试集）。

    | Algorithm | error |
    | --------- | ----- |
    | A         | 3%    |
    | B         | 5%    |

    因此我们为误差率引入惩罚项$ w^{(i)} $:

    $$ error = \frac{1}{m_{dev}} \sum_{i=1}^{m_{dev}} w^{(i)} L\{y_{pred}^{(i)} \neq y^{(i)} \}  $$

    $$ w^{(i)} = \begin{cases}
        1 & x\ is \ non\_porn \\
        10 & x\ is \ porn 
    \end{cases} $$

    这样对色情图片的惩罚赋予了更大的权重，当算法错误的将色情图片识别为猫时，会得到更大的误差。
    当然，评估完成后我们可以改变模型的代价函数，让模型能够识别色情图片：

    $$ J = \frac{1}{\sum w^{(i)}} \sum_{i=1}^m w^{(i)} L(\hat{y}, y) $$

2. 当在目前的开发集和测试集上评估无法正确预测模型的实际表现时，例如，下面的误差率是在高质量的图片开发集或测试集上获得的，然而当实际部署至手机应用时，因为都是一些模糊的图片，本应表现良好的模型A实际表现不好，这时应该修改开发集和测试集，重新评估。
   
    | Algorithm | error |
    | --------- | ----- |
    | A         | 3%    |
    | B         | 5%    |

## 偏差与方差分析

### 误差定义

1. 贝叶斯误差：$ B_{error} $
2. 人类表现误差：$ H_{error} $
3. 训练集误差：$ T_{error} $
4. 开发集误差：$ D_{error} $
5. 偏差：$ T_{error} - H_{error} $
6. 方差：$ D_{error} - T_{error} $

一般来说：$ H_{error} \approx B_{error} $

### 高偏差或高方差

1. 高偏差
   
   $ T_{error} - B_{error} $ 较高

2. 高方差

    $ D_{error} - T_{error} $ 较高

### 减小偏差或方差

1. 检查是否存在高偏差问题，若存在：
   
   - 增大训练数据，或：
   - 增大网络，或
   - 替换优化算法，或
   - 超参数优化，或
   - 寻找更适合的神经网络
  
2. 检查是否存在高方差问题，若存在：
   
   - 取得更多数据，或：
   - 正则化，或
   - 寻找更适合的神经网络

## 误差分析

### 找出主要错误

给出所有分类错误的图片，并记录每一类错误的占比：

| Image      | Dog | Great Cats | Blury | Instagrum |
| ---------- | --- | ---------- | ----- | --------- |
| 1          | ✓   |            |       |           |
| 2          |     |            | ✓     |           |
| 3          |     | ✓          | ✓     |           |
| 4          |     | ✓          | ✓     | ✓         |
| ...        | ... | ...        | ...   | ...       |
| % of total | 8%  | 43%        | 61%   | 12%       |

根据错误比例以及问题处理的难度决定是否应该花费时间处理这一类错误。

### 错误标记处理

深度学习算法对随机错误很稳健，通常不用专门处理错误标记。但对系统误差很敏感（比如：一直将狗标记为猫）。主要取决于开发集误差中，错误标记造成的误差所占的比例。若比例过高，那很有必要处理错误标记，反之，错误标记可能不是当下首要解决的问题。

## 数据集划分

1. 开发集和测试集必须满足同分布，一般来说是把来自不同分布的数据随机打乱然后分为开发集和测试集
2. 开发集和测试集的大小

    | 数据量     | 训练集                    | 开发集  | 测试集  |
    | ---------- | ------------------------- | ------- | ------- |
    | 100-10,000 | 60%                       | 20%     | 20%     |
    | 100-10,000 | 60%                       | \       | 40%     |
    | > 1000,000 | D(all) - D(dev) - D(test) | ~10,000 | ~10,000 |

    以上划分并不是约定，只是说明当数据量足够大时，开发集和测试集只需占一小部分即可

### 分布不同时的训练集与开发/测试集划分

针对训练集与开发/测试集分布不同的情况，

- 方案一：可以尝试将二者进行混合，例如，将200000张来自网页的训练集图片与10000张来自移动应用的开发/测试集图片混合后进行重新划分：

    | train  | dev  | test |
    | ------ | ---- | ---- |
    | 205000 | 2500 | 2500 |

    这样虽保证了数据集均源自同一分布，但有**个巨大的缺点**：

    开发/测试集中来自移动应用的图片期望为：$ 119 = 2500 * \frac{10000}{210000} $

    即最终将花大量时间来优化来自网页的图片时模型的表现。

- 方案二：从来自移动应用的图片中分一半加入到训练集中，而剩下的一半分割为开发/测试集

    | dataset | web    | mobile |
    | ------- | ------ | ------ |
    | train   | 200000 | 5000   |
    | dev     |        | 2500   |
    | test    |        | 2500   |

    虽然方案二使训练集与开发/测试集的数据分布不同，但长期来看，效果更佳。

## 分布不同时的偏差与方差分析

### 问题引出

假设训练集与开发集分布不同，
人类表现误差接近于0，
训练集误差：1%
开发集误差：10%

此时有9%的方差，但由于分布不同，无法确定9%的方差有多少是因为算法未接触开发集而影响了方差，又有多少是由于开发集的数据分布不同。

### 问题处理

为了辨别这两个问题，引入新的数据集：训练-开发集(traning-dev set)，该数据集与训练集同分布，同时引入新的误差：训练-开发误差($ TD_{error} $)。

重新定义方差：$ TD_{error} - T_{error} $
数据不匹配率：$ D_{error} - TD_{error} $

1. 高方差问题
   
    | dataset     | error |
    | ----------- | ----- |
    | Traning     | 1%    |
    | Traning-dev | 9%    |
    | Dev         | 10%   |

    在上面的例子中，方差很大，说明这是一个高方差问题。

2. 失配问题

    | dataset     | error |
    | ----------- | ----- |
    | Traning     | 1%    |
    | Traning-dev | 1.5%  |
    | Dev         | 10%   |

    在上面的例子中，数据不匹配率很大，说明这是数据不匹配问题。

3. 高偏差问题

    | dataset     | error |
    | ----------- | ----- |
    | Human       | 0%    |
    | Traning     | 9%    |
    | Traning-dev | 11%   |
    | Dev         | 12%   |

    上面的例子中，偏差过大，这是高偏差问题

### 失配问题的处理过程

1. 进行误差分析
   
   即利用开发集对模型验证，分析产生的误差中误差的主要成分，观察与训练集的不同。
   例如在语音识别任务中，开发集的语音伴随各种白噪音。

2. 搜集更多与开发集相似的数据

    比如说，语音识别任务中，开发集中的语音伴随这车载噪音，则我们可以通过人工数据合成在训练集中模拟出车载噪音。

## 迁移学习与多任务学习

### 迁移学习

通过将其它任务上学习到的底层信息，应用于另一个不同的任务，可以大大减少数据的收集，并加快训练速度。

限制：

1. 只有两个任务的输入相同时，才能使用迁移学习
2. 任务A的数据量需要远大于任务B的数据量（假设从A迁移至B）

### 多任务学习

多任务学习通过尝试让一个神经网络同时做多个任务，每个任务学习到的信息同时会帮助其它任务

限制：

1. 要训练的多个任务共享一些低层次的特征
2. （非硬性）每个任务的数据量是相似的
3. 网络足够大

## 端到端学习

通过大量数据的学习，直接建立X到Y的映射。

